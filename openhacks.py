# -*- coding: utf-8 -*-
"""OpenHacks.ipynb

Automatically generated by Colaboratory.

Authors: 
Debaleen Das Spandan (https://github.com/the-it-weirdo)
Shouvit Pradhan (https://github.com/shaw8wit)

Original file is located at
    https://colab.research.google.com/drive/1xGb6Uftf29cdgyZrm1YbmbjRDyN7vFMy
"""

pip install vaderSentiment

import json
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
import re
import string
import pandas as pd
import tweepy
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# if using drive, uncomment the following 2 lines
'''from google.colab import drive
drive.mount('/content/drive')'''

'''file_path = "filepath/data.json"
json_data = open(file_path)
data = json.load(json_data)'''

# using twitter api to get data

# using twitter api to get data


CONSUMER_KEY = "XXX"
CONSUMER_SECRET = "XXX"
ACCESS_TOKEN = "XXX"
ACCESS_TOKEN_SECRET= "XXX"

def twitter_auth_connection():
    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)
    auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)
    return tweepy.API(auth)

def twitter_data(search):
    api = twitter_auth_connection()
    tweets = []
    for tweet in api.search(q='#{search}', lang="en", rpp=5, tweet_mode="extended"):
        tweetObj ={}
        tweetObj["tweet"] = tweet.full_text
        tweetObj["location"] = tweet.user.location
        tweetObj["retweet_count"] = tweet.retweet_count
        tweetObj['id'] = tweet.id
        tweets.append(tweetObj)
    return tweets

def extend_stopwords(theList, character):
  a = []
  for i in theList:
    i = i[:0] + character + i[0:]
    a.append(i) # appending "'character'whatever" for e.g. #whatever
    i = i[0:] + '\n\n' # for words ending with newline characters
    a.append(i) # appending e.g. '#whatever\n\n'
    i = i[0:] + '\n' # for words ending with newline characters
    a.append(i) # appending e.g. '#whatever\n'
  return a

hashtag_re = re.compile("(?:^|\s)[＃#]{1}(\w+)", re.UNICODE)
mention_re = re.compile("(?:^|\s)[＠ @]{1}([^\s#<>[\]|{}]+)", re.UNICODE)
url_re = r"(?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:'\".,<>?«»“”‘’]))"

def filter_data(tweet_str):
  hashtag_list = re.findall(hashtag_re, tweet_str)
  mention_list = re.findall(mention_re, tweet_str)
  url = re.findall(url_re, tweet_str)
  url_list = [x[0] for x in url]
  stopwords_list = stopwords.words('english')
  stopwords_list.extend(extend_stopwords(hashtag_list, "#")) # extending stopwords list to include # tags for the particular tweet
  stopwords_list.extend(extend_stopwords(hashtag_list, "\n#")) # extending stopwords list to include # tags in new line for the particular tweet
  stopwords_list.extend(extend_stopwords(hashtag_list, "\n\n#")) # extending stopwords list to include # tags in new line for the particular tweet
  stopwords_list.extend(extend_stopwords(mention_list, "@")) # extending stopwords list to include @ mentions for the particular tweet
  stopwords_list.extend(extend_stopwords(mention_list, "\n@")) # extending stopwords list to include @ mentions for the particular tweet starting in newline
  stopwords_list.extend(extend_stopwords(mention_list, "\n\n@")) # extending stopwords list to include @ mentions for the particular tweet starting in newline
  stopwords_list.extend(extend_stopwords(url_list, "\n\n")) # extending stopwords list to include include urls for the particular tweet starting in newline
  stopwords_list.extend(extend_stopwords(url_list, "\n")) # extending stopwords list to include include urls for the particular tweet starting in newline
  stopwords_list.extend(url_list) # extending stopwords list to include urls for the particular tweet
  stopwords_list.append('\n')
  stopwords_list.append('RT')
  stopwords_list.append('\n\n')
  stopwords_list.append('&amp;')
  #print(stopwords_list)
  filtered_words_temp = [word for word in tweet_str.split(" ") if word not in stopwords_list] # removing hashtags, mentions, links and stopwords
  #print(' '.join(filtered_words_temp))
  res = re.sub('['+string.punctuation+']', '', ' '.join(filtered_words_temp)).split() # removing punctuations
  #print(res)
  filtered_words = [word for word in res if word.lower() not in stopwords_list]
  return (hashtag_list, mention_list, url_list, filtered_words)

data = twitter_data('apple')

tags = []
mentions = []
urls = []
filtered_words_ls = []
locs = []
tweets = []
retweet_count = []
for i in data:
  tweet_str = i['tweet']
  a, c, d, b = filter_data(tweet_str)
  tags.insert(0, a)
  mentions.insert(0, c)
  urls.insert(0, d)
  filtered_words_ls.insert(0, b)
  tweets.insert(0, tweet_str)
  #retweet_count.insert(0, i['retweet_count'])
  locs.insert(0, i['location'])

myDict = {}
myDict['tags'] = tags
myDict['mentions'] = mentions
myDict['filtered_words'] = filtered_words_ls
myDict['locations'] = locs
myDict['urls'] = urls
#myDict['retweet_count'] = retweet_count 
myDict['original_tweet'] = tweets


dataframe = pd.DataFrame.from_dict(myDict)

# we don't need to read data from csv as we are using twitter api to fetch data and then converting it to dataframe.

'''csv_path = "filepath/data.csv"
# dataframe.to_csv(csv_path)'''
#datas = pd.read_csv(csv_path)

analyzer = SentimentIntensityAnalyzer()

pos = []
neg = []
neu = []

def get_word_sentiment(listof, val):
  pos_word_list=[]
  neu_word_list=[]
  neg_word_list=[]
  for word in listof:
    if (analyzer.polarity_scores(word)['compound']) >= 0.1:
      pos_word_list.append(word)
    elif (analyzer.polarity_scores(word)['compound']) <= -0.1:
      neg_word_list.append(word)
    else:
      neu_word_list.append(word)
  # print('Positive:',pos_word_list)        
  # print('Neutral:',neu_word_list)    
  # print('Negative:',neg_word_list)
  return pos_word_list if val == 1 else neg_word_list if val == -1 else neu_word_list

answers = []
for i in range(len(dataframe['original_tweet'])):
  ans = analyzer.polarity_scores(dataframe['original_tweet'][i])
  answers.append(ans)
  #print(ans)
  if (ans['compound'] > 0.25):
    pos += get_word_sentiment(dataframe['filtered_words'][i],1)
  elif (ans['compound'] < -0.25):
    neg += get_word_sentiment(dataframe['filtered_words'][i],-1)
  else:
    neu += get_word_sentiment(dataframe['filtered_words'][i],0)


myDict['sentiments'] = answers

output_file = "filepath/output.json"
outfile = open(output_file, "w")
json_obj = json.dumps(myDict)
outfile.write(json_obj)
outfile.close()

f = open(output_file, "r")
print(f.read())

